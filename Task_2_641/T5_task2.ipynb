{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcyi4yIsOHkC",
        "outputId": "24a079a3-2552-404c-a9c0-33bedeb20469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m205.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install tensorflow\n",
        "!pip install tf-keras\n",
        "!pip install datasets\n",
        "import argparse\n",
        "import json\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import PorterStemmer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6aY2iwLtW6n"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.7.0 'torch_xla[tpu]==2.7.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv91KTDpstnx"
      },
      "outputs": [],
      "source": [
        "# !pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIpRGX9gaFDe"
      },
      "outputs": [],
      "source": [
        "# !add-apt-repository ppa:graphics-drivers/ppa\n",
        "# !apt update\n",
        "# !apt install nvidia-384 nvidia-384-dev\n",
        "# !apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev\n",
        "# !pip install torch torchvision torchaudio torch_xla -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64\n",
        "# !wget https://developer.nvidia.com/compute/cuda/9.2/Prod/patches/1/cuda-repo-ubuntu1710-9-2-local-cublas-update-1_1.0-1_amd64\n",
        "# !dpkg -i cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64\n",
        "# !apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "# !apt-get update\n",
        "# !apt-get install cuda\n",
        "# !dpkg -i cuda-repo-ubuntu1710-9-2-local-cublas-update-1_1.0-1_amd64\n",
        "# !apt-get install nvidia-cuda-toolkit\n",
        "# !pip install torch torchvision torchaudio\n",
        "# import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxF8Ce0YZiGN"
      },
      "outputs": [],
      "source": [
        "# # !pip3 uninstall torch torchvision torchaudio\n",
        "# # !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "# import torch\n",
        "# print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0MUzvhOPS0f"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "# Specifying the File Path\n",
        "train_data_dir = '/content/drive/My Drive/641/train.jsonl'\n",
        "val_data_dir = '/content/drive/My Drive/641/val.jsonl'\n",
        "test_data_dir = '/content/drive/My Drive/641/test.jsonl'\n",
        "output_dir = '/content/drive/My Drive/641/spoiler_result.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmvXXVCFO8QC"
      },
      "outputs": [],
      "source": [
        "def load_preprocess_data(file_path, test_flag):\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as inp:\n",
        "        # Read input data\n",
        "        data = {\n",
        "            'id': [],\n",
        "            'title': [],\n",
        "            'text': [],\n",
        "            'label': [],\n",
        "            'target': []\n",
        "        }\n",
        "\n",
        "        # extract data from jsonl file\n",
        "        for i in inp:\n",
        "            i = json.loads(i)\n",
        "            if test_flag:\n",
        "                data['id'].append(i['id'])\n",
        "            else:\n",
        "                data['id'].append(''.join(i['postId']))\n",
        "            data['title'].append(''.join(i['postText']))\n",
        "            data['text'].append(''.join(i['targetParagraphs']))\n",
        "\n",
        "            if not test_flag:\n",
        "                data['label'].append(''.join(i['tags']))\n",
        "                data['target'].append(''.join(i['spoiler']))\n",
        "\n",
        "        # assign corresponding value to input, output & id\n",
        "            # clean text\n",
        "        texts = [preprocess_text(text) for text in data['text']]\n",
        "        titles = [preprocess_text(title) for title in data['title']]\n",
        "        if not test_flag:\n",
        "          spoilers = [preprocess_text(target) for target in data['target']]\n",
        "\n",
        "\n",
        "        if debug:\n",
        "            print(f\"Sample spoiler: {spoilers[0:5]}\")\n",
        "\n",
        "        if not test_flag:\n",
        "          data_dict = {\n",
        "              \"input_text\" : texts,\n",
        "              \"input_title\" : titles,\n",
        "              \"target_text\" : spoilers\n",
        "          }\n",
        "        else: # for test set\n",
        "          data_dict = {\n",
        "              \"input_text\" : texts,\n",
        "              \"input_title\" : titles,\n",
        "          }\n",
        "\n",
        "        dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "76b3pz9wPA-x"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # # Convert to lowercase\n",
        "    # text = text.lower()\n",
        "    # # # Remove special characters (keep only letters, numbers, spaces & some special punctuations)\n",
        "    # text = re.sub(r'[^a-zA-Z0-9\\s,\\\"\\?]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    if stopword_removal:\n",
        "        # remove stop words\n",
        "        tokenised = tokenize(text)\n",
        "        sw = load_stopword()\n",
        "        cleaned = remove_stopwords(tokenised, sw)\n",
        "        text = \" \".join(token for token in cleaned)\n",
        "\n",
        "    return text\n",
        "def tokenize(text):\n",
        "    \"\"\"Tokenize the input text by removing punctuation and splitting into words.\"\"\"\n",
        "\n",
        "    tokenized_text = []\n",
        "\n",
        "    for word in re.findall(r'\\w+', text):\n",
        "        tokenized_text.append(word)\n",
        "\n",
        "    return tokenized_text\n",
        "\n",
        "\n",
        "def load_stopword():\n",
        "    res = []\n",
        "    if stopword_ref == 'nltk':\n",
        "        res = stopwords.words('english')\n",
        "    else:\n",
        "        stopword_set = []\n",
        "        with open('../stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                stopword_set.append(line.strip())\n",
        "\n",
        "        for item in stopword_set:\n",
        "            for word in re.findall(r'\\w+', item):\n",
        "                # Key: \\w matches any alphanumeric character and underscore\n",
        "                res.append(word)\n",
        "        res = set(res)\n",
        "        # if debug:\n",
        "        #     print(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "def remove_stopwords(tokens, stopwords):\n",
        "\n",
        "    # reduce word to simplest root form\n",
        "    porter = PorterStemmer()\n",
        "    cleaned_tokens = [porter.stem(word) for word in tokens if word.lower() not in stopwords]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "\n",
        "def save_model(model, filename):\n",
        "    \"\"\"Save the trained model to a file.\"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    if debug:\n",
        "        print(f\"Model saved to {filename}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qXJOnl3QQmlo"
      },
      "outputs": [],
      "source": [
        "# for testing google pegasus\n",
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HSUrbK3ZONDg"
      },
      "outputs": [],
      "source": [
        "debug = True\n",
        "retrain = True\n",
        "stopword_removal = False  # True, False\n",
        "stopword_ref = 'nltk'  # nltk, pre\n",
        "ref_x = 'text'  # title, text\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "model_name = 't5-base'  # t5-small, t5-base, t5-large\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "LEARNING_RATE = 1e-2\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VAL_BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 2\n",
        "WEIGHT_DECAY = 2\n",
        "LOGGING_STEPS = 10\n",
        "\n",
        "# LEARNING_RATE = 3e-4\n",
        "# TRAIN_BATCH_SIZE = 8\n",
        "# VAL_BATCH_SIZE = 8\n",
        "# NUM_EPOCHS = 2\n",
        "# WEIGHT_DECAY = 0.3\n",
        "# LOGGING_STEPS = 10\n",
        "\n",
        "# LEARNING_RATE = 8e-5\n",
        "# TRAIN_BATCH_SIZE = 8\n",
        "# VAL_BATCH_SIZE = 8\n",
        "# NUM_EPOCHS = 5\n",
        "# WEIGHT_DECAY = 0.1\n",
        "# LOGGING_STEPS = 10\n",
        "# question_prefix = \"Question: what is the following inferring in passage:\"\n",
        "\n",
        "# LEARNING_RATE = 4e-5\n",
        "# TRAIN_BATCH_SIZE = 8\n",
        "# VAL_BATCH_SIZE = 8\n",
        "# NUM_EPOCHS = 5\n",
        "# WEIGHT_DECAY = 0.05\n",
        "# LOGGING_STEPS = 10\n",
        "\n",
        "# LEARNING_RATE = 2e-5\n",
        "# TRAIN_BATCH_SIZE = 8\n",
        "# VAL_BATCH_SIZE = 8\n",
        "# NUM_EPOCHS = 3\n",
        "# WEIGHT_DECAY = 0.01\n",
        "# LOGGING_STEPS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvw7j-XVO_if"
      },
      "outputs": [],
      "source": [
        "# Question: Find in passage, what is '{title}' inferring in passage? Passage: {text}\n",
        "# Question: What is the key message that '{title}' is inferring in passage? Passage: {text}\n",
        "#\n",
        "# Current Best\n",
        "# Question: What is the key spoiler that '{title}' is inferring in passage? Passage: {text}\n",
        "def preprocess_dataset(dataset):\n",
        "\n",
        "    inputs = [f\"Question: What is the key spoiler that '{title}' is inferring in passage? Passage: {text}\"\n",
        "              for title, text in zip(dataset['input_title'], dataset['input_text'])]\n",
        "    targets = [spoiler for spoiler in dataset['target_text']]\n",
        "\n",
        "    # prepare data for model\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "    model_targets = tokenizer(targets, max_length=30, truncation=True, padding='max_length')\n",
        "    model_inputs[\"labels\"] = model_targets[\"input_ids\"]\n",
        "\n",
        "    return  model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7Fo9B96NSh5"
      },
      "outputs": [],
      "source": [
        "# clean text & return a dataset (w/ text, title, target)\n",
        "train_dataset = load_preprocess_data(train_data_dir, test_flag=False)\n",
        "val_dataset = load_preprocess_data(val_data_dir, test_flag=False)\n",
        "\n",
        "\n",
        "# apply tokenization and preprocessing of dataset\n",
        "tokenized_train = train_dataset.map(preprocess_dataset, batched = True)\n",
        "tokenized_val = val_dataset.map(preprocess_dataset, batched = True)\n",
        "\n",
        "\n",
        "if debug:\n",
        "    print(f\"Sample tokenized train input: {tokenized_train[0]['input_text']}\")\n",
        "    print(f\"Sample tokenized train target: {tokenized_train[0]['target_text']}\")\n",
        "\n",
        "# =========================================================================\n",
        "# training\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate= LEARNING_RATE,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size= VAL_BATCH_SIZE,\n",
        "    num_train_epochs= NUM_EPOCHS,\n",
        "    weight_decay= WEIGHT_DECAY,\n",
        "    logging_dir= \"logs\",\n",
        "    logging_steps= LOGGING_STEPS,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "# ==========================================================================\n",
        "\n",
        "# testing\n",
        "with open(test_data_dir, 'r', encoding=\"utf8\") as inp, open(output_dir, 'w', encoding=\"utf8\") as out:\n",
        "    # Read input data\n",
        "    data = {\n",
        "        'id': [],\n",
        "        'title': [],\n",
        "        'text': [],\n",
        "        'label': [],\n",
        "        'target': []\n",
        "    }\n",
        "\n",
        "    # extract data from jsonl file\n",
        "    for i in inp:\n",
        "        i = json.loads(i)\n",
        "        data['id'].append(i['id'])\n",
        "        data['title'].append(''.join(i['postText']))\n",
        "        data['text'].append(''.join(i['targetParagraphs']))\n",
        "\n",
        "    texts = [preprocess_text(text) for text in data['text']]\n",
        "    titles = [preprocess_text(title) for title in data['title']]\n",
        "    id = [id for id in data['id']]\n",
        "\n",
        "    # prepare inputs for the model\n",
        "    inputs = [f\"Question: What is the key spoiler that '{title}' is inferring in passage? Passage: {text}\" for title, text in zip(titles, texts)]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    # Generate predictions\n",
        "    predictions = model.generate(input_ids=torch.tensor(model_inputs['input_ids']).to('cuda'))\n",
        "\n",
        "    # Decode predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    count = 0\n",
        "    # Write results to output file\n",
        "    for i, pred in enumerate(decoded_preds):\n",
        "        if count == 0:\n",
        "          print(f'input sample: {inputs[0]}')\n",
        "          print(f'prediction sample: {pred}')\n",
        "          count = 1\n",
        "        out.write(json.dumps({'id': id[i], 'spoiler': pred}) + '\\n')\n",
        "\n",
        "    # with open(output_dir, 'r') as f:\n",
        "    #   for line in f:\n",
        "    #       data = json.loads(line)\n",
        "    #       predictions.append(data)\n",
        "\n",
        "    #   # Create DataFrame\n",
        "    #   df = pd.DataFrame(predictions)\n",
        "\n",
        "    #   # Save as CSV\n",
        "    #   df.to_csv('/content/drive/My Drive/641/task2_submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}